# Trainer Meeting, 17 July 2025, UTC 14:00 
See this link for your local time: https://www.timeanddate.com/worldclock/fixedtime.html?iso=20250717T14 

Attending:   

    Rob Davey (he/him) / The Carpentries

    Toby Hodges (he/him) / The Carpentries 

    Amanda Kis (she/her) / University of Oklahoma

    Angelique Trusler, The Carpentries

    Jesse Sadler (he/him), Virginia Tech, Blacksburg, USA

    Scott Peterson (he/him), University at Buffalo

    Olexandr Konovalov (he/him), University of St Andrews

    SherAaron Hurt, The Carpentries

    Jordan Pedersen (she/her), University of Guelph, Canada

    Kari L. Jordan

    Ben Chiewphasa (he/him), Columbia University

    Holly Bik (she/her), University of Georgia

    Vasileios Lenis, University of Birmingham

    Sarah Stevens (she/her/hers), University of Wisconsin-Madison

    Margareth Gfrerer, Kotbe University of Education, Addis Ababa


Reminder to host: claim 'host' role in Zoom room and enable waiting room if desired

Notetaker: Amanda Kis

Agenda:

    Introductions

    Favorite summer pastime right now?

    Special Topic: LLM and generative AI

    Blog posts on AI: https://carpentries.org/blog/tag/artificial-intelligence/

    Some questions we might consider:

    A mental model of computational research methods remains essential, even if knowledge of the specific syntax of a given programming language is becoming less important. Should this change our teaching approach in Carpentries workshops?

    Is participatory live coding/demo still the most effective method to teach what learners need to learn in 2025 and beyond?

    Should Instructor Training be modified to equip Instructors with knowledge and methods needed to be able to answer questions and offer guidance on the usage of generative AI in their workshops?


 LLM and generative AI discussion

     Resources to recommend to send to graduate students?

    What do you trust?

    What do you need to be wary of?

    The Bullshit Machines by Carl T. Bergstrom and Jevin D. West: https://thebullshitmachines.com/

    The same authors wrote a book, Calling bullshit: the art of skepticism in a data-driven world

    AI Snake Oil: https://www.aisnakeoil.com/

    See if your library has a guide

    FIU has curated AI LibGuides from various institutions https://library.fiu.edu/ai-infolit/libguides

    For some neuroscience/mathematics/social takes on AI, I recommend the papers and resources from Iris van Rooij and Olivia Guest. They are critical, but from a domain expert perspective.

    An open letter from educators who refuse the call to adopt GenAI in education https://openletter.earth/an-open-letter-from-educators-who-refuse-the-call-to-adopt-genai-in-education-cb4aee75

    AI Literacy Framework from EU/OECD: https://ailiteracyframework.org/about/

    Skeptical about changing the Carpentries trainings, since they are introductory level

    We're trying to get learners to understand what a coding project looks like—organizational aspect of coding (e.g., code and data should be in the same folder)

    There are more and more scripts being created, but where they are and how they're related to other things isn't there

    Some open source projects are also rejecting/refusing contributions that are AI generated, not just for ethical reasons but for copyright reasons also: https://github.com/qemu/qemu/blob/master/docs/devel/code-provenance.rst#use-of-ai-content-generators

    Do we need to emphasis the beginner topics? ie... unix shell should be more basic

    We need to reassure instructors what we're teaching is still needed

    We are teaching fundamental topics like reproducible computational analysis, how to troubleshoot, how to structure a project, how to understand syntax, not specifically Python or R only

    Maybe this needs to be more explicit in the lessons—instructors bring those skills in, but they aren't necessarily explicit in the lessons

    AI can help you code, but it won't be really helpful unless you can understand and do these basic things on your own

    Do you need to invest the time to learn the language? Do you need to get something done? It's useful to differentiate the purpose

    The latter is where the demand for AI is coming from

    AI won't replace foundational knowledge

    Grad students are increasingly required to span languages to get a workflow to run

    Lots of demand for AI to debug

    "Is the Carpentries way of teaching the antithesis to Vibe coding?"

    You tell AI want you want and it spits it out and they don't know what it means themselves

    There needs to be a human to debug what AI has written and check it

    Vibe coders may have little or no experience of coding or software engineering, and they use AI to generate swaths of code but have no debugging skills, no understanding of testing frameworks

    The carpentries promotes checking work—is that the antithesis of vibe coding?

    Hallucinating malicious packages: https://datavizs25.classes.andrewheiss.com/resource/ai-bs.html#vibe-coding-and-learning

    Mental modeling with AI at the beginning of workshops

    Maybe providing some guiding questions could help so that the convo doesn’t derail

    Resarch data management - best practices for starting a project, shared drives, where is the data being put so it's reusable, FAIR, file naming conventions

    Carpentries can facilitate

    Files and directories are not something as familiar to newer learners than more experienced learners (e.g., files are in Google Drive)

    These fundamental concepts aren't obvious to a lot of learners

    Since the paper on searching for files came out I’ve changed how I describe folder structure, I talk about how we all usually search for files in google drive and how we can encode meaning into the folder system that will be helpful for us and others going forward.  That seemed novel to them.

    https://www.theverge.com/22684730/students-file-folder-directory-structure-education-gen-z

    We have to acknowledge that people have gen AI available and are using it and it's affecting how they do software development

    We can give advice about when it's useful and not

    Be prepared for this to come up, whether you bring it up in workshops or not

    We are equipped to answer these questions, and the community can support!

    Does this need to be put into Instructor Training? Does it make sense as sections in lessons? Do materials need to be created that can be used regardless of workshop?

    Maybe something like a Carpentries community guide for instructors that has some advice when these questions come up or how to integrate it into lessons

    Even if we don’t talk about it a lot in instructor training, we can point folks to it.

    Maybe an LLM FAQ

    Worth exploring putting something in Instructor Training

    Have there been questions about AI in Instructor Training?

    I really like this idea - but I think it could also be useful for learners too. "The Carpentries expectations for Instructors and Learners" - outline some of the discussion points we've had up-front so some event expectations can be managed to some extent

    Easy for blog posts we market to be lost in all the information

    A combination of all three is useful so information doesn't get lost

    Not everyone who uses our materials come to our workshops—they also are self studying

    Advice in lessons for those who are self studying

    I think there is some convo about where to put it in each lesson in the #genai-lesson-updates slack channel

    Maybe this is a good time to develop a standalone lesson on the proper use of gen AI, limitations

    Maybe not enough for a workshop

    If we don't talk about AI, everyone assumes everyone is doing it

    If you're depending on AI, your learning is affected

    It's hard and slow to learn, but the challenge is worth it and part of the learning!

    Not everything has to be quick just because we have AI

    Think about, why are you doing this and learning this?

    Google Maps analogy in the curriculum (sense of direction decreases as we use Google Maps) (when we're being driven around, unless we pay strict attention, we don't know where we are)

    Phone number analogy—we used to remember phone numbers, now we don't

    We emphasize going slow to develop skills, as opposed to using AI to get the quick win

    Emphasize in workshops

    Carpentries website on best practices or current thoughts on LLMs

    We can point others to this resource


# Trainer Meeting 17 July 2025, UTC 23:00 
See this link for your local time: https://www.timeanddate.com/worldclock/fixedtime.html?iso=20250717T23

Attending:   

    Amanda Kis (she/her) / University of Oklahoma

    Jesse Sadler (he/him), Virginia Tech, Blacksburg, USA

    Murray Cadzow (he/him) University of Otago, NZ

    Jon Wheeler (he/him), University of New Mexico

    Karen Cranston (she/her), University Health Network, Toronto, Canada

    Erin Graham (she/her), James Cook University, Australia

    Giorgia Mori (she/her), Australian BioCommons


     


Reminder to host: claim 'host' role in Zoom room and enable waiting room if desired

Notetaker: Amanda Kis

Agenda:

    Introductions

    Icebreaker: Favorite place to watch the sunset

    Special Topic: LLM and generative AI

    Blog posts on AI: https://carpentries.org/blog/tag/artificial-intelligence/

    Some questions we might consider:

    A mental model of computational research methods remains essential, even if knowledge of the specific syntax of a given programming language is becoming less important. Should this change our teaching approach in Carpentries workshops?

    Is participatory live coding/demo still the most effective method to teach what learners need to learn in 2025 and beyond?

    Should Instructor Training be modified to equip Instructors with knowledge and methods needed to be able to answer questions and offer guidance on the usage of generative AI in their workshops?


 LLM and generative AI discussion

     is there a summary of the morning? 

    if someone comes to us looking for resources, what should we tell them? 

    should resources go into instructor training, or in workshops themselves?

    What would materials look like that the Carpentries can point people to?

    Are they for instructors?

    Are they for learners?

    What is an overall statement the Carpentries can make about LLMs?

    What is the value of Carpentries workshops in a gen AI world?

    Build foundations to help people use LLMs in useful ways

    Carpentries and make / link to resources, but we need to keep in mind that lessons used in workshops and for self-study

    Is there a phased approach? (e.g., short statement included in lessons first, progress to discussion to when and when not to use them, progress to do we incorporate some use of LLMs and gen AI in our lessons and workshops

    this exists in a few lessons already, e.g. https://swcarpentry.github.io/python-novice-gapminder/04-built-in.html#generative-ai

    One potential path: Having somewhere with a general statement on AI and how we're thinking about it in terms of our learning philosophies (not a blog post, which tends to get lost)

    Is there a place it could be put in Instructor Training as a discussion, as part of our philosophy of teaching?

    This would take some time to compose!

    But it feels necessary at this point in time

    Then it could filter down into individual workshops

    There are ethical issues with endorsing AI

    Practical: What advice do we give to instructors?

    If we ignore it, we become irrelevant

    Set-up instructors are generic, so that might be a good place for a statement on AI about how we engage with AI in workshops

    Want to be intentional about how we talk about AI

    many people who come to the Carpentries have no coding experience at all (even in instructor training)

    Is it overwhelming when we get into technicalities and ethics in Instructor training? Is it increasing cognitive load?

    Want to think about how we want to present it and where to begin

    distinction between technical workshops (how we use AI) and instructor training (how we talk about AI)

    What about AI is relevant to a particular mode of instruction?

    Does AI have a place in beginner workshops besides a mention? We're trying to teach them the fundamentals of coding and data science

    Everyone talks about ChatGPT in general, but there are other AI tools that are more specific to coding (e.g., GitHub Copilot)

    Hard to find information about how to implement AI as a partner for coding projects (this could be a Carpenties niche?)

    When asking a question, a student will show me what ChatGPT said and ask what we think

    "People are going to ask about AI - in technical workshops or in IT - what should we tell them?"

    We do need to keep in mind that the technology changes rapidly, and we don't want to create something (e.g., statement) that is so specific that it needs constant revision

    there is a big change in access to knowledge and how you get knowledge

    should workshops move away from syntax to a more conceptual/higher-level framework focusing on building blocks?

    if copilot gives you the syntax, how do you test and problem solve?

    How do I figure out what's correct?

    From Sarah earlier meeting: The Carpentries teaches reproducible research, not R and Python specifically

    What's the value added of the Carpentries? It's not R and Python syntax

    If you don't have those foundations, it's hard to interpret and connect what LLMs give you

    Learning skills vs. getting something done—what's the goal?

    The Carpntries is trying to help learners learn skills

    Are the skills we're teaching needing to change?

    Our technical workshops are focused on languages in many cases

    We don't focus as much on the skill of information acquision and learning

    Novice coders who are novices at AI using AI to code is different than an expert using AI to assist with coding

    This affects how we talk about AI

    Jon was able to use AI to make research more reproducible (longer process to change a workflow than asking for a piece of code)

    Authentic tasks: AI expertise, coding expertise quadrants (you need to be more cautious about using AI depending on the quadrant you're in)

    How do we use AI to improve reproducibility of research? That might be important to discuss when talking about and demonstrating AI

    Is the Carpentries about making research more reproducible? We do this, but this isn't how we get people into workshops. We get them into workshops by promoting efficiency.

    We get into workshops by helping learners be more efficient in their research by using tools well

    The reproducibility is like a bonus on top of the efficiency

    AI has a lot of temptation for efficiency

    Efficiency and reproducibility are both in the Carpentries mission ("MIssion: The Carpentries empowers a diverse global community by equipping individuals with essential data and computational skills. We promote efficient, open, and reproducible research practices.")

    Novices can get AI to do anything and everything, and you don't know why it's working (if it is) or how it's working but you get your results, but this is circumventing learning

    Could have part of a workshop on generating AI code—now what? How do I test it? How do I make sure it does what I want? How do I make sure it gives correct results?

    Example of a (very old SWC) workshop: Not live coding, but instead go through pre-written code line and by line, have them make modifications, ask questions, make it better

    Could do something similar with AI-generated code

    How do we teach them to think critically about what AI is doing and giving us? What are the skills that people need now to do research and be efficient? Is it syntax? Is it how to efficiently troubleshoot? Is it how to evaluate AI generated code?

    Need a literature review on how the use of AI affects learning to inform Instructor training

    A group needs to do a deep dive (e.g., instructor trainers, maintainers)

    We want whatever we do to be backed up by evidence, like the rest of the instructor training is

    Tying together earlier and later discussions

    Some instructors of technical workshops will be more into AI and others who will be less into AI

    There's no ignoring that these questions will be asked during our workshops

    We as trainers need to slowly build up materials to ensure as many people as possible have some resources to have an answer for as many of those questions as possible

    Resources, general thinking more than technical details

    Our workshops are short, and we can't control what students do after workshops, so we need to have answers

    In IT material, guiding people towards things you need to consider  before engaging with AI:

    ethical AI, institutional policies

    noting that this list is also going to change frequently

    Not only the technical side of AI is changing: How we think about and use it is changing, too, so our recommendations will change

    How it's being used at institutions is changing (e.g., now there are entire assignments that hinge on using AI)

    What are the next steps?

    What should the position of the Carpentries as an organization be? How will they engage with AI?

    How do we handle this when they have so many ethical issues? How does it vibe with our organizational values?

    We can't really be an absolute "no" since we are going to get questions, and we can't restrict students from using them

    The next step might be thinking about answers to these questions

    Consider how to use AI safely

    And some of our instructors to use AI in their own coding, and that probably is going to increase, and they'll bring that perspective to technical workshops

    Instructors have autonomy on this in workshops

    Using AI is like using Google to find an answer


